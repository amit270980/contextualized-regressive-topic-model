"""
This is a modified version of file main.py (stable v2.2.0)
The modifications are to incorporate an extra, "regressive" term in the training objective
All are highlighted in the code
Authors: Amit Kumar, Nazanin Esmaili, Massimo Piccardi, November 2022
"""

import warnings
warnings.filterwarnings('ignore')
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import torch
torch.manual_seed(1234)

#import mlflow.pytorch.autolog()
#import mlflow.pytorch._pytorch_autolog
# -*- coding: utf-8 -*-

"""Combined TM on Wikipedia Data (Preproc+Saving+Viz) (stable v2.2.0)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fXJjr_rwqvpp1IdNQ4dxqN4Dp88cxO97

# Tutorial: Combined Topic Modeling

(last updated 10-05-2021)

In this tutorial, we are going to use our **Combined Topic Model** to get the topics out of a collections of articles.

## Topic Models

Topic models allow you to discover latent topics in your documents in a completely unsupervised way. Just use your documents and get topics out.

## Contextualized Topic Models

![](https://raw.githubusercontent.com/MilaNLProc/contextualized-topic-models/master/img/logo.png)

What are Contextualized Topic Models? **CTMs** are a family of topic models that combine the expressive power of BERT embeddings with the unsupervised capabilities of topic models to get topics out of documents.

## Python Package

You can find our package [here](https://github.com/MilaNLProc/contextualized-topic-models).

![https://github.com/MilaNLProc/contextualized-topic-models/actions](https://github.com/MilaNLProc/contextualized-topic-models/workflows/Python%20package/badge.svg) ![https://pypi.python.org/pypi/contextualized_topic_models](https://img.shields.io/pypi/v/contextualized_topic_models.svg) ![https://pepy.tech/badge/contextualized-topic-models](https://pepy.tech/badge/contextualized-topic-models)

# **Before you start...**

If you have additional questions about these topics, follow the links:

- you need to work with languages different than English: [click here!](https://contextualized-topic-models.readthedocs.io/en/latest/language.html#language-specific)
- you can't get good results with topic models: [click here!](https://contextualized-topic-models.readthedocs.io/en/latest/faq.html#i-am-getting-very-poor-results-what-can-i-do)
- you want to load your own embeddings: [click here!](https://contextualized-topic-models.readthedocs.io/en/latest/faq.html#can-i-load-my-own-embeddings)

# Enabling the GPU

First, you'll need to enable GPUs for the notebook:

- Navigate to Edit→Notebook Settings
- select GPU from the Hardware Accelerator drop-down

[Reference](https://colab.research.google.com/notebooks/gpu.ipynb)

# Installing Contextualized Topic Models

First, we install the contextualized topic model library
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install contextualized-topic-models==2.2.0

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pyldavis
import pandas as pd
from IPython.utils import capture
from docutils.nodes import raw

"""## Restart the Notebook

For the changes to take effect, we now need to restart the notebook.

From the Menu:

Runtime → Restart Runtime

# Data

We are going to need some data. You should upload a file with one document per line. We assume you haven't run any preprocessing script.

However, if you want to first test the model without uploading your data, you can simply use the test file I'm putting here
"""

##################################################################

# This is for a command-line argument (extra hyperparameter epsilon, >= 0)

import argparse

parser = argparse.ArgumentParser(description='The Contextualized Regressive Topic Model')
parser.add_argument('--epsilon', type=float, default=0.0, help='coefficient to mix the new RL_bert loss')

args = parser.parse_args()
print("Epsilon:",args.epsilon)

##################################################################

"""# Importing what we need"""

from contextualized_topic_models.models.ctm import CombinedTM
from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation
from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessing
import nltk

nltk.download('stopwords')

##################################################################

# This is to read the three datasets used in the paper:

import pandas as pd

# Reading the Wiki20K dataset:

import wget
url = 'https://raw.githubusercontent.com/vinid/data/master/dbpedia_sample_abstract_20k_unprep.txt'
text_file = wget.download(url)
# !head -n 2 dbpedia_sample_abstract_20k_unprep.txt
text_file = "dbpedia_sample_abstract_20k_unprep.txt" # EDIT THIS WITH THE FILE YOU UPLOAD
documents = [line.strip() for line in open(text_file, encoding="utf-8").readlines()]

# Reading the 20 Newsgroups dataset:

# df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')
# docs = df.content.values.tolist()
# # documents = docs[0:5000] # <--- for smaller-size experiments

# Reading the Amazon Fine Food Reviews dataset (from local .csv file):

# df = pd.read_csv('Reviews.csv')
# # documents = df['Text'][:25000] # <--- for smaller-size experiments

print("Number of documents:", len(documents))

sp = WhiteSpacePreprocessing(documents, stopwords_language='english')
preprocessed_documents, unpreprocessed_corpus, vocab = sp.preprocess()

# print("vocabulary length", len(vocab))
# preprocessed_documents[:2]

"""We don't discard the non-preprocessed texts, because we are going to use them as input for obtaining the contextualized document representations.

Let's pass our files with preprocess and unpreprocessed data to our `TopicModelDataPreparation` object. This object takes care of creating the bag of words for you and of obtaining the contextualized BERT representations of documents. This operation allows us to create our training dataset.

Note: Here we use the contextualized model "paraphrase-distilroberta-base-v1".

"""

tp = TopicModelDataPreparation("paraphrase-distilroberta-base-v1")

training_dataset = tp.fit(text_for_contextual= unpreprocessed_corpus, text_for_bow=preprocessed_documents)

"""Let's check the first ten words of the vocabulary """

# tp.vocab[:10]

"""## Training our Combined TM

Finally, we can fit our new topic model. We will ask the model to find 50 topics in our collection.
"""

####################################################
# Note the extra keyword argument, epsilon:
####################################################

ctm = CombinedTM(bow_size=len(tp.vocab), contextual_size=768, n_components=20, num_epochs=100,dropout=0.1, epsilon=args.epsilon) 
ctm.fit(training_dataset) # run the model

"""# Topics

After training, now it is the time to look at our topics: we can use the

```
get_topic_lists
```

function to get the topics. It also accepts a parameter that allows you to select how many words you want to see for each topic.

If you look at the topics, you will see that they all make sense and are representative of a collection of documents that comes from Wikipedia (general knowledge). Notice that the topics are in English, because we trained the model on English documents.
"""

ctm.get_topic_lists(5)

"""# Let's Draw!

We can use PyLDAvis to plot our topic in a nice and friendly manner :)
"""

# lda_vis_data = ctm.get_ldavis_data_format(tp.vocab, training_dataset, n_samples=20)
#
# import pyLDAvis as vis
#
# lda_vis_data = ctm.get_ldavis_data_format(tp.vocab, training_dataset, n_samples=10)
#
# ctm_pd = vis.prepare(**lda_vis_data)
# print(vis.display(ctm_pd))

"""# Topic Predictions

Ok now we can take a document and see which topic has been assigned to it. Results will obviously change with respect to the documents you are using. For example, let's predict the topic of the first preprocessed document that is talking about a peninsula.
"""

topics_predictions = ctm.get_thetas(training_dataset, n_samples=5) # get all the topic predictions

preprocessed_documents[0] # see the text of our preprocessed document

import numpy as np
topic_number = np.argmax(topics_predictions[0]) # get the topic id of the first document

#topic_number

#print(ctm.get_topic_lists(5))


# ctm.get_topic_lists(5)[18]
# ctm.get_topic_lists(5)[topic_number] #and the topic should be about natural location related things

"""# Save Our Model for Later Use"""

ctm.save(models_dir="./")

# # let's remove the trained model
# del ctm
#
# ctm = CombinedTM(bow_size=len(tp.vocab), contextual_size=768, num_epochs=100, n_components=50)
#
# #ctm.load("contextualized_topic_model_nc_50_tpm_0.0_tpv_0.98_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99",epoch=19)
# ctm.load("contextualized_topic_model_nc_20_tpm_0.0_tpv_0.95_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99",epoch=9)
#
# print(ctm.get_topic_lists(5))

# if __name__ == "__main__":
#    main()

from contextualized_topic_models.evaluation.measures import CoherenceNPMI, CoherenceCV, InvertedRBO, TopicDiversity
texts = [doc.split() for doc in preprocessed_documents]
npmi = CoherenceNPMI(texts=texts, topics=ctm.get_topic_lists(10))
score_npmi = npmi.score()

CV = CoherenceCV(texts=texts, topics=ctm.get_topic_lists(10))
score_CV = CV.score()

RBO = TopicDiversity(topics=ctm.get_topic_lists(25))
score_RBO = RBO.score()

print("Topic coherence NPMI:", "{:.5f}".format(score_npmi))
print("Topic coherence CV:", "{:.5f}".format(score_CV))
print("Topic Diversity:", "{:.5f}".format(score_RBO))

#print(ctm.get_topic_lists(5))